{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.  Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy."
      ],
      "metadata": {
        "id": "r06lZLQeqd2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Decision Trees\n",
        "# Replacing 'base_estimator' with 'estimator' to be compatible with newer scikit-learn versions\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "id": "1L6W3BrNRLxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "ZozQLno6q56H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Regressor with Decision Trees\n",
        "# Replacing 'base_estimator' with 'estimator' to be compatible with newer scikit-learn versions\n",
        "bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Model MSE: {mse:.2f}')"
      ],
      "metadata": {
        "id": "elTfyk5QSJn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores."
      ],
      "metadata": {
        "id": "U9US579ASOlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importances = pd.DataFrame({'Feature': data.feature_names, 'Importance': rf_clf.feature_importances_})\n",
        "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print feature importance scores\n",
        "print(feature_importances)\n"
      ],
      "metadata": {
        "id": "QciC38OySANt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Train a Random Forest Regressor and compare its performance with a single Decision Tree."
      ],
      "metadata": {
        "id": "MhMT6-OlSmEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Single Decision Tree Regressor\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "dt_reg.fit(X_train, y_train)\n",
        "y_pred_dt = dt_reg.predict(X_test)\n",
        "dt_mse = mean_squared_error(y_test, y_pred_dt)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Decision Tree MSE: {dt_mse:.2f}\")\n",
        "print(f\"Random Forest MSE: {rf_mse:.2f}\")"
      ],
      "metadata": {
        "id": "xVZV-5ZhSgVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier."
      ],
      "metadata": {
        "id": "OSPgnvyrS3iI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier with OOB score enabled\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Print the OOB score\n",
        "print(f\"Out-of-Bag (OOB) Score: {rf_clf.oob_score_:.2f}\")"
      ],
      "metadata": {
        "id": "7MlkDAhLS0N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Train a Bagging Classifier using SVM as a base estimator and print accuracy."
      ],
      "metadata": {
        "id": "Ro51O10rTIdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with SVM as base estimator\n",
        "# Replacing 'base_estimator' with 'estimator' to be compatible with newer scikit-learn versions\n",
        "bagging_clf = BaggingClassifier(estimator=SVC(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "id": "HRAZgDBgTj_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Train a Random Forest Classifier with different numbers of trees and compare accuracy."
      ],
      "metadata": {
        "id": "pTX5XwFLTqcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of different numbers of trees to test\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "\n",
        "# Train and evaluate Random Forest with different numbers of trees\n",
        "for n_estimators in n_estimators_list:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f'Number of Trees: {n_estimators}, Accuracy: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "id": "0jUeklhDTYdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score."
      ],
      "metadata": {
        "id": "un9gcdW5UGlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Logistic Regression as base estimator\n",
        "bagging_clf = BaggingClassifier(estimator=LogisticRegression(solver='lbfgs', max_iter=1000),\n",
        "                                n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_prob = bagging_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate AUC score\n",
        "auc_score = roc_auc_score(y_test, y_pred_prob)\n",
        "print(f'Model AUC Score: {auc_score:.2f}')\n"
      ],
      "metadata": {
        "id": "pE5BDHI7UbxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Train a Random Forest Regressor and analyze feature importance scores."
      ],
      "metadata": {
        "id": "w3yiOL6kX251"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "import pandas as pd\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importances = pd.DataFrame({'Feature': [f'Feature {i}' for i in range(X.shape[1])],\n",
        "                                    'Importance': rf_reg.feature_importances_})\n",
        "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print feature importance scores\n",
        "print(feature_importances)\n"
      ],
      "metadata": {
        "id": "jvyEfXnrXVNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Train an ensemble model using both Bagging and Random Forest and compare accuracy."
      ],
      "metadata": {
        "id": "m3emlJVhYQt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Bagging Regressor\n",
        "bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_reg.predict(X_test)\n",
        "bagging_mse = mean_squared_error(y_test, y_pred_bagging)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Bagging Regressor MSE: {bagging_mse:.2f}\")\n",
        "print(f\"Random Forest Regressor MSE: {rf_mse:.2f}\")\n"
      ],
      "metadata": {
        "id": "hCRxgxzYZJ4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  Train a Random Forest Classifier and tune hyperparameters using GridSearchCV."
      ],
      "metadata": {
        "id": "3hcWbMlUZjV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform Grid Search with Cross Validation\n",
        "grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_rf_clf = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_rf_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Best Parameters: {grid_search.best_params_}')\n",
        "print(f'Best Model Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "id": "PtnaUBuiYhMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Train a Bagging Regressor with different numbers of base estimators and compare performance."
      ],
      "metadata": {
        "id": "Jp-6qnbcaHDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of different numbers of base estimators to test\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "\n",
        "# Train and evaluate Bagging Regressor with different numbers of estimators\n",
        "for n_estimators in n_estimators_list:\n",
        "    bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=n_estimators, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f'Number of Estimators: {n_estimators}, MSE: {mse:.2f}')\n"
      ],
      "metadata": {
        "id": "7r0-SQbwZxfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  Train a Random Forest Classifier and analyze misclassified samples."
      ],
      "metadata": {
        "id": "N4AdAw5AakDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Identify misclassified samples\n",
        "misclassified_indices = np.where(y_test != y_pred)[0]\n",
        "print(f'Number of Misclassified Samples: {len(misclassified_indices)}')\n",
        "print('Indices of Misclassified Samples:', misclassified_indices)\n"
      ],
      "metadata": {
        "id": "kf76UHXzaiCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.  Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier."
      ],
      "metadata": {
        "id": "LTbewE_Aa9LX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree Classifier\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "y_pred_dt = dt_clf.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Train a Bagging Classifier with Decision Tree as base estimator\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_clf.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(f'Decision Tree Accuracy: {dt_accuracy:.2f}')\n",
        "print(f'Bagging Classifier Accuracy: {bagging_accuracy:.2f}')\n"
      ],
      "metadata": {
        "id": "DP4k_NRHa6d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Train a Random Forest Classifier and visualize the confusion matrix."
      ],
      "metadata": {
        "id": "gPRwgGyzbgE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eYso2DlXbe_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy."
      ],
      "metadata": {
        "id": "BFeIELCbqPJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base classifiers\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "svm = SVC(probability=True, random_state=42)\n",
        "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train individual classifiers and evaluate performance\n",
        "for clf, name in zip([dt, svm, logreg], [\"Decision Tree\", \"SVM\", \"Logistic Regression\"]):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(f\"{name} Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# Define stacking classifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base classifiers\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "svm = SVC(probability=True, random_state=42)\n",
        "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train individual classifiers and evaluate performance\n",
        "for clf, name in zip([dt, svm, logreg], [\"Decision Tree\", \"SVM\", \"Logistic Regression\"]):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(f\"{name} Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# Define stacking classifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[('dt', dt), ('svm', svm), ('logreg', logreg)],\n",
        "    final_estimator=LogisticRegression(max_iter=1000, random_state=42))\n",
        "\n",
        "# Train and evaluate stacking classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test)\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy_score(y_test, y_pred_stack):.4f}\")\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test)\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy_score(y_test, y_pred_stack):.4f}\")\n"
      ],
      "metadata": {
        "id": "22iWvbKQcBbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.  Train a Random Forest Classifier and print the top 5 most important features."
      ],
      "metadata": {
        "id": "DqZ-o9BEq88f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base classifiers\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "svm = SVC(probability=True, random_state=42)\n",
        "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train individual classifiers and evaluate performance\n",
        "for clf, name in zip([dt, svm, logreg], [\"Decision Tree\", \"SVM\", \"Logistic Regression\"]):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(f\"{name} Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# Define stacking classifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base classifiers\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "svm = SVC(probability=True, random_state=42)\n",
        "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train individual classifiers and evaluate performance\n",
        "for clf, name in zip([dt, svm, logreg], [\"Decision Tree\", \"SVM\", \"Logistic Regression\"]):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(f\"{name} Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# Define stacking classifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[('dt', dt), ('svm', svm), ('logreg', logreg)],\n",
        "    final_estimator=LogisticRegression(max_iter=1000, random_state=42)\n",
        ")\n",
        "\n",
        "# Train and evaluate stacking classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test)\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy_score(y_test, y_pred_stack):.4f}\")\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf.feature_importances_\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Print top 5 most important features\n",
        "important_features = sorted(zip(feature_importances, feature_names), reverse=True)[:5]\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "for importance, name in important_features:\n",
        "    print(f\"{name}: {importance:.4f}\")\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test)\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy_score(y_test, y_pred_stack):.4f}\")\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf.feature_importances_\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Print top 5 most important features\n",
        "important_features = sorted(zip(feature_importances, feature_names), reverse=True)[:5]\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "for importance, name in important_features:\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "NFTSGxYQrIIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.  Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score."
      ],
      "metadata": {
        "id": "d3AEEcY6rgRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base classifiers\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "svm = SVC(probability=True, random_state=42)\n",
        "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train individual classifiers and evaluate performance\n",
        "for clf, name in zip([dt, svm, logreg], [\"Decision Tree\", \"SVM\", \"Logistic Regression\"]):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(f\"{name} Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# Define stacking classifier (Fixed Syntax Error)\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[('dt', dt), ('svm', svm), ('logreg', logreg)],\n",
        "    final_estimator=LogisticRegression(max_iter=1000, random_state=42)\n",
        ")\n",
        "\n",
        "# Train and evaluate stacking classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test)\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy_score(y_test, y_pred_stack):.4f}\")\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf.feature_importances_\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Print top 5 most important features\n",
        "important_features = sorted(zip(feature_importances, feature_names), reverse=True)[:5]\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "for importance, name in important_features:\n",
        "    print(f\"{name}: {importance:.4f}\")\n",
        "\n",
        "# Train a Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_clf.predict(X_test)\n",
        "\n",
        "# Evaluate Bagging Classifier performance\n",
        "precision = precision_score(y_test, y_pred_bagging)\n",
        "recall = recall_score(y_test, y_pred_bagging)\n",
        "f1 = f1_score(y_test, y_pred_bagging)\n",
        "\n",
        "print(\"Bagging Classifier Performance:\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "MptBx9K4rwee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy."
      ],
      "metadata": {
        "id": "jTN8c2g5tPu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base classifiers\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "svm = SVC(probability=True, random_state=42)\n",
        "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train individual classifiers and evaluate performance\n",
        "for clf, name in zip([dt, svm, logreg], [\"Decision Tree\", \"SVM\", \"Logistic Regression\"]):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(f\"{name} Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# Define stacking classifier (Fixed Syntax Error)\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[('dt', dt), ('svm', svm), ('logreg', logreg)],\n",
        "    final_estimator=LogisticRegression(max_iter=1000, random_state=42)\n",
        ")\n",
        "\n",
        "# Train and evaluate stacking classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test)\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy_score(y_test, y_pred_stack):.4f}\")\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf.feature_importances_\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Print top 5 most important features\n",
        "important_features = sorted(zip(feature_importances, feature_names), reverse=True)[:5]\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "for importance, name in important_features:\n",
        "    print(f\"{name}: {importance:.4f}\")\n",
        "\n",
        "# Train a Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_clf.predict(X_test)\n",
        "\n",
        "# Evaluate Bagging Classifier performance\n",
        "precision = precision_score(y_test, y_pred_bagging)\n",
        "recall = recall_score(y_test, y_pred_bagging)\n",
        "f1 = f1_score(y_test, y_pred_bagging)\n",
        "\n",
        "print(\"Bagging Classifier Performance:\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "# Analyze the effect of max_depth on accuracy\n",
        "max_depth_values = range(1, 21)\n",
        "accuracy_scores = []\n",
        "\n",
        "for max_depth in max_depth_values:\n",
        "    rf = RandomForestClassifier(n_estimators=100, max_depth=max_depth, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Plot max_depth vs accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(max_depth_values, accuracy_scores, marker='o', linestyle='dashed', color='b')\n",
        "plt.xlabel('Max Depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Effect of max_depth on Random Forest Accuracy')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mDoHXyxaso8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.  Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare\n",
        "performance."
      ],
      "metadata": {
        "id": "ELqdnlcqt-0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In the BaggingRegressor initialization, replace 'base_estimator' with 'estimator"
      ],
      "metadata": {
        "id": "bJJWZ1HJvdrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, BaggingClassifier, BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.datasets import load_breast_cancer, load_diabetes\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load classification dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base classifiers\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "svm = SVC(probability=True, random_state=42)\n",
        "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train individual classifiers and evaluate performance\n",
        "for clf, name in zip([dt, svm, logreg], [\"Decision Tree\", \"SVM\", \"Logistic Regression\"]):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(f\"{name} Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# Define stacking classifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[('dt', dt), ('svm', svm), ('logreg', logreg)],\n",
        "    final_estimator=LogisticRegression(max_iter=1000, random_state=42)\n",
        ")\n",
        "\n",
        "# Train and evaluate stacking classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test)\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy_score(y_test, y_pred_stack):.4f}\")\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf.feature_importances_\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Print top 5 most important features\n",
        "important_features = sorted(zip(feature_importances, feature_names), reverse=True)[:5]\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "for importance, name in important_features:\n",
        "    print(f\"{name}: {importance:.4f}\")\n",
        "\n",
        "# Train a Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42) # Replacing 'base_estimator' with 'estimator' to be compatible with newer scikit-learn versions\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_clf.predict(X_test)\n",
        "\n",
        "# Evaluate Bagging Classifier performance\n",
        "precision = precision_score(y_test, y_pred_bagging)\n",
        "recall = recall_score(y_test, y_pred_bagging)\n",
        "f1 = f1_score(y_test, y_pred_bagging)\n",
        "\n",
        "print(\"Bagging Classifier Performance:\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "# Load regression dataset\n",
        "data_reg = load_diabetes()\n",
        "X_reg, y_reg = data_reg.data, data_reg.target\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Regressor with Decision Tree and KNeighbors\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "knn_reg = KNeighborsRegressor()\n",
        "\n",
        "bagging_dt = BaggingRegressor(estimator=dt_reg, n_estimators=50, random_state=42)\n",
        "bagging_knn = BaggingRegressor(estimator=knn_reg, n_estimators=50, random_state=42)\n",
        "\n",
        "bagging_dt.fit(X_train_reg, y_train_reg)\n",
        "bagging_knn.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Predictions\n",
        "y_pred_dt = bagging_dt.predict(X_test_reg)\n",
        "y_pred_knn = bagging_knn.predict(X_test_reg)\n",
        "\n",
        "# Evaluate performance\n",
        "mae_dt = mean_absolute_error(y_test_reg, y_pred_dt)\n",
        "mse_dt = mean_squared_error(y_test_reg, y_pred_dt)\n",
        "mae_knn = mean_absolute_error(y_test_reg, y_pred_knn)\n",
        "mse_knn = mean_squared_error(y_test_reg, y_pred_knn)\n",
        "\n",
        "print(\"Bagging Regressor Performance:\")\n",
        "print(f\"Decision Tree - MAE: {mae_dt:.4f}, MSE: {mse_dt:.4f}\")\n",
        "print(f\"KNeighbors - MAE: {mae_knn:.4f}, MSE: {mse_knn:.4f}\")\n"
      ],
      "metadata": {
        "id": "tUGxGPectjY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.  Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score."
      ],
      "metadata": {
        "id": "UiT9ud2wwIKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_probs = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Plot the ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f}')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QxH0bBRtwoSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.  Train a Bagging Classifier and evaluate its performance using cross-validation\n"
      ],
      "metadata": {
        "id": "qyO63MqEys5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Bagging Classifier\n",
        "# Replacing 'base_estimator' with 'estimator' to be compatible with newer scikit-learn versions\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "scores = cross_val_score(bagging_clf, X_train, y_train, cv=5, scoring='accuracy')"
      ],
      "metadata": {
        "id": "MNB62sA-zZlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "scores = cross_val_score(bagging_clf, X_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print cross-validation scores\n",
        "print(f\"Cross-Validation Accuracy Scores: {scores}\")\n",
        "print(f\"Mean Accuracy: {scores.mean():.4f}\")\n",
        "\n",
        "# Train on full training data and evaluate on test data\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "tLeoFPNoyk6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Train a Random Forest Classifier and plot the Precision-Recall curve"
      ],
      "metadata": {
        "id": "v_ePBMi_z5dJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_probs = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate Precision-Recall curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
        "avg_precision = average_precision_score(y_test, y_probs)\n",
        "\n",
        "# Plot the Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f'PR Curve (AP = {avg_precision:.4f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PYnmyUKJy_-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy."
      ],
      "metadata": {
        "id": "dMNS9Qmp0V4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base and meta classifiers\n",
        "base_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "meta_clf = LogisticRegression()\n",
        "\n",
        "# Train individual Random Forest Classifier\n",
        "base_clf.fit(X_train, y_train)\n",
        "y_pred_base = base_clf.predict(X_test)\n",
        "accuracy_base = accuracy_score(y_test, y_pred_base)\n",
        "print(f\"Random Forest Accuracy: {accuracy_base:.4f}\")\n",
        "\n",
        "# Train a Stacking Classifier\n",
        "stacking_clf = StackingClassifier(estimators=[('rf', base_clf)], final_estimator=meta_clf, passthrough=True)\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test)\n",
        "accuracy_stack = accuracy_score(y_test, y_pred_stack)\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy_stack:.4f}\")\n"
      ],
      "metadata": {
        "id": "2QY3k4hE0ROx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Train a Bagging Regressor with different levels of bootstrap samples and compare performance."
      ],
      "metadata": {
        "id": "1ZRl_VNj0o8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define different bootstrap sample sizes\n",
        "bootstrap_samples = [0.5, 0.7, 1.0]\n",
        "errors = []\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for sample_size in bootstrap_samples:\n",
        "    bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=100,\n",
        "                                   max_samples=sample_size, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    errors.append(mse)\n",
        "    print(f\"Bagging Regressor (Bootstrap Sample = {sample_size}) MSE: {mse:.4f}\")\n",
        "\n",
        "# Plot performance comparison\n",
        "plt.bar([str(s) for s in bootstrap_samples], errors, color=['blue', 'green', 'red'])\n",
        "plt.xlabel('Bootstrap Sample Size')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Performance of Bagging Regressor with Different Bootstrap Samples')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T04KS54u0l0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Afw-ulY81B49"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}