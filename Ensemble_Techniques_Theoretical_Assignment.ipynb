{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theoretical"
      ],
      "metadata": {
        "id": "hT-NzdUbe9-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Can we use Bagging for regression problems?\n"
      ],
      "metadata": {
        "id": "DOz78L6UfHED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Bagging is widely used for classification and regression tasks. Classification helps improve the accuracy of predictions by combining the outputs of multiple classifiers trained on different subsets of the data."
      ],
      "metadata": {
        "id": "QKZh_SHRil65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  What is the difference between multiple model training and single model training ?\n"
      ],
      "metadata": {
        "id": "wLeKw2hefHVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Single model training involves training one model for a specific task, while multiple model training involves training multiple models simultaneously or sequentially to improve performance, robustness, or adaptability."
      ],
      "metadata": {
        "id": "a-dIpOuqit9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain the concept of feature randomness in Random Forest.\n"
      ],
      "metadata": {
        "id": "ITuzsHcHfHZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Feature randomness in Random Forest refers to the technique of selecting a random subset of features at each split in a decision tree, ensuring diversity among trees and reducing overfitting."
      ],
      "metadata": {
        "id": "pjly_0z6ixt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is OOB (Out-of-Bag) Score?"
      ],
      "metadata": {
        "id": "PhLXot9tfHcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : The Out-of-Bag (OOB) score is an estimate of a Random Forest model's accuracy, calculated using the predictions on training samples that were not included in the bootstrap sampling for each tree."
      ],
      "metadata": {
        "id": "Wr9OixTTjKh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  How can you measure the importance of features in a Random Forest model?"
      ],
      "metadata": {
        "id": "aasR0b9JjKXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Feature importance in a Random Forest model can be measured using metrics like Gini importance (mean decrease in impurity) or permutation importance, which assesses the impact of each feature on model accuracy by randomly shuffling its values."
      ],
      "metadata": {
        "id": "Wci_5gpJjck5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  Explain the working principle of a Bagging Classifier."
      ],
      "metadata": {
        "id": "vgHIajsvfHfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : A Bagging Classifier works by training multiple instances of the same base model on different bootstrap samples (randomly drawn subsets with replacement) of the training data and then aggregating their predictions—using majority voting for classification or averaging for regression—to improve accuracy and reduce variance."
      ],
      "metadata": {
        "id": "n70_nKRFjtpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How do you evaluate a Bagging Classifier’s performance?"
      ],
      "metadata": {
        "id": "WQbGM9oPfHi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : A Bagging Classifier’s performance is evaluated using metrics like accuracy, precision, recall, F1-score, ROC-AUC for classification tasks, and mean squared error (MSE) or R-squared for regression, often validated through cross-validation or Out-of-Bag (OOB) score estimation."
      ],
      "metadata": {
        "id": "AezRAHJ4fHmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. How does a Bagging Regressor work?"
      ],
      "metadata": {
        "id": "TZ0L_c0JfHvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : A Bagging Regressor works by training multiple instances of the same regression model on different bootstrap samples of the training data and averaging their predictions to reduce variance and improve stability."
      ],
      "metadata": {
        "id": "RZywkurGkPHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main advantage of ensemble techniques?"
      ],
      "metadata": {
        "id": "dHni_wtBk8aQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : The main advantage of ensemble techniques is that they combine multiple models to improve predictive accuracy, reduce overfitting, and enhance model robustness compared to individual models."
      ],
      "metadata": {
        "id": "nX791tRvlCgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the main challenge of ensemble methods?"
      ],
      "metadata": {
        "id": "eWCOdypAlB-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : The main challenge of ensemble methods is their increased computational complexity and difficulty in interpretability compared to single models, as they require training multiple models and aggregating their predictions."
      ],
      "metadata": {
        "id": "gbzQi7Onla67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Explain the key idea behind ensemble techniques."
      ],
      "metadata": {
        "id": "c8TE-7Oslnkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : The key idea behind ensemble techniques is to combine multiple diverse models to make more accurate and robust predictions by reducing variance, bias, or both."
      ],
      "metadata": {
        "id": "uZ1Wiupcl1xE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is a Random Forest Classifier?"
      ],
      "metadata": {
        "id": "0nxUQbhGl52U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : A Random Forest Classifier is an ensemble learning algorithm that builds multiple decision trees using random subsets of data and features, then combines their predictions through majority voting to improve accuracy and reduce overfitting."
      ],
      "metadata": {
        "id": "YYgtkjd0l_-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  What are the main types of ensemble techniques?"
      ],
      "metadata": {
        "id": "Xd_tcwWamLYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : The main types of ensemble techniques are **Bagging** (Bootstrap Aggregating), **Boosting** (e.g., AdaBoost, Gradient Boosting), **Stacking** (stacked generalization), and **Voting/Averaging**, each designed to improve model performance by combining multiple learners."
      ],
      "metadata": {
        "id": "pVwezaSzmTIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is ensemble learning in machine learning?"
      ],
      "metadata": {
        "id": "1H64vvn3mrvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Ensemble learning in machine learning is a technique that combines multiple models to improve predictive performance, robustness, and generalization compared to individual models."
      ],
      "metadata": {
        "id": "4DvVstmWmz7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. When should we avoid using ensemble methods?"
      ],
      "metadata": {
        "id": "s9IFY7Kem-MA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Ensemble methods should be avoided when computational resources are limited, model interpretability is crucial, or when a single model already provides sufficient accuracy without significant overfitting."
      ],
      "metadata": {
        "id": "LilJw-WBnEZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  How does Bagging help in reducing overfitting?"
      ],
      "metadata": {
        "id": "ek2507e6na2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Bagging reduces overfitting by training multiple models on different bootstrap samples of the data and averaging their predictions, which lowers variance and increases generalization."
      ],
      "metadata": {
        "id": "TMTuVfpang0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.  Why is Random Forest better than a single Decision Tree?"
      ],
      "metadata": {
        "id": "IVwhKoAbnt9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Random Forest is better than a single Decision Tree because it reduces overfitting, improves accuracy, and increases robustness by combining multiple trees trained on different subsets of data and features."
      ],
      "metadata": {
        "id": "3V1qDmMKn3AU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the role of bootstrap sampling in Bagging?"
      ],
      "metadata": {
        "id": "hx77TA_doBeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Bootstrap sampling in Bagging creates diverse training subsets by randomly selecting data points with replacement, allowing each model to learn from slightly different data and reducing variance for improved generalization."
      ],
      "metadata": {
        "id": "que3_ghmoGLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What are some real-world applications of ensemble techniques?"
      ],
      "metadata": {
        "id": "8_sQ2F5robSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Ensemble techniques are used in real-world applications such as fraud detection, medical diagnosis, stock market prediction, image recognition, sentiment analysis, and recommendation systems to enhance accuracy and robustness."
      ],
      "metadata": {
        "id": "95g57ARZogn0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What is the difference between Bagging and Boosting?"
      ],
      "metadata": {
        "id": "Nq2e6Qr3o0D3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Bagging reduces variance by training multiple models independently on bootstrap samples and averaging their predictions, while Boosting reduces bias by sequentially training models, where each model corrects the errors of the previous one."
      ],
      "metadata": {
        "id": "p8NOCgIDo4jW"
      }
    }
  ]
}